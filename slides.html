<!DOCTYPE html>
<html>
  <head>
    <title>Working with tidymodels</title>
    <meta charset="utf-8">
    <meta name="author" content="Emil Hvitfeldt" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="theme.css" type="text/css" />
    <link rel="stylesheet" href="colors.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, title-slide

# Working with tidymodels
## OCRUG meetup
### Emil Hvitfeldt
### 2019-1-29

---




`tidymodels` is a "meta-package" for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.

.left-column[
![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tidymodels.png)
]

.right-column[

```r
library(tidymodels)
```


```r
## ✔ broom     0.5.1          ✔ purrr     0.3.0     
## ✔ dials     0.0.2          ✔ recipes   0.1.4     
## ✔ dplyr     0.7.8          ✔ rsample   0.0.4     
## ✔ ggplot2   3.1.0          ✔ tibble    2.0.1     
## ✔ infer     0.4.0          ✔ yardstick 0.0.2     
## ✔ parsnip   0.0.1.9000
```
]

---

# The packages

.pull-left[
- broom
- dials
- dplyr
- ggplot2
- infer
- parsnip
]

.pull-right[
- purrr
- recipes
- rsample
- tibble
- yardstick
]

---

# The packages (tidyverse)

.pull-left[
- broom
- dials
- **dplyr**
- **ggplot2**
- infer
- parsnip
]

.pull-right[
- **purrr**
- recipes
- rsample
- **tibble**
- yardstick
]

---

# The packages (tidyverse)

.pull-left[
- broom
- dials
- **.light[dplyr]**
- **.light[ggplot2]**
- infer
- parsnip
]

.pull-right[
- **.light[purrr]**
- recipes
- rsample
- **.light[tibble]**
- yardstick
]

---

# The packages

.pull-left[
- **.light[broom]**
- **.light[dials]**
- **.light[dplyr]**
- **.light[ggplot2]**
-  **.light[infer]**
- **parsnip**
]

.pull-right[
- **.light[purrr]**
- **recipes**
- **rsample**
- **.light[tibble]**
- **yardstick**
]

---

# ⚠️ Disclaimer ⚠️

This talk is not designed to give opinions with respect to modeling best practices.

This talk is designed to showcase what packages are available and what they can do.

---

# Consider 32 cars from 1973-74


```r
head(mtcars)
```

```
##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
```

---


```r
model_glm &lt;- glm(am ~ disp + drat + qsec, data = mtcars, 
                 family = "binomial")
```

--


```r
predict(model_glm)
```

```
##           Mazda RX4       Mazda RX4 Wag          Datsun 710 
##           105.97448            69.85214            27.10173 
##      Hornet 4 Drive   Hornet Sportabout             Valiant 
##          -276.59440          -240.20025          -313.42683 
##          Duster 360           Merc 240D            Merc 230 
##          -158.99087          -123.81721          -284.08255 
##            Merc 280           Merc 280C          Merc 450SE 
##           -20.37716           -59.07966          -167.78204 
##          Merc 450SL         Merc 450SLC  Cadillac Fleetwood 
##          -180.68287          -206.48454          -458.77204 
## Lincoln Continental   Chrysler Imperial            Fiat 128 
##          -427.72554          -357.75792            27.25037 
##         Honda Civic      Toyota Corolla       Toyota Corona 
##           164.39652            20.76278           -90.84576 
##    Dodge Challenger         AMC Javelin          Camaro Z28 
##          -211.90064          -189.27739           -74.78345 
##    Pontiac Firebird           Fiat X1-9       Porsche 914-2 
##          -297.35324            63.64819           184.39936 
##        Lotus Europa      Ford Pantera L        Ferrari Dino 
##           146.50220            24.28831           162.60217 
##       Maserati Bora          Volvo 142E 
##            21.69348            33.80864
```

---


```r
library(glmnet)
model_glmnet &lt;- glmnet(am ~ disp + drat + qsec, data = mtcars, 
                       family = "binomial")
```

--


```
## Error in glmnet(am ~ disp + drat + qsec, data = mtcars, family = "binomial"): unused argument (data = mtcars)
```

--

&lt;br&gt;


```r
model_glmnet &lt;- glmnet(x = as.matrix(mtcars[, c("disp", "drat", "qsec")]),
                       y = mtcars[, "am"],
                       family = "binomial")
```

--

&lt;br&gt;


```r
model_glm &lt;- glm(x = as.matrix(mtcars[, c("disp", "drat", "qsec")]),
                 y = mtcars[, "am"],
                 family = "binomial")
```

--


```
## Error in environment(formula): argument "formula" is missing, with no default
```

---

# User-facing problems in modeling in R

- Data must be a matrix (except when it needs to be a data.frame)
- Must use formula or x/y (or both)
- Inconsistent naming of arguments (ntree in randomForest, num.trees in ranger)
- na.omit explicitly or silently
- May or may not accept factors

--

.center[![](https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/240/apple/155/tired-face_1f62b.png)]

---

# Syntax for Computing Predicted Class Probabilities

|Function     |Package      |Code                                       |
|:------------|:------------|:------------------------------------------|
|`lda`        |`MASS`       |`predict(obj)`                             |
|`glm`        |`stats`      |`predict(obj, type = "response")`          |
|`gbm`        |`gbm`        |`predict(obj, type = "response", n.trees)` |
|`mda`        |`mda`        |`predict(obj, type = "posterior")`         |
|`rpart`      |`rpart`      |`predict(obj, type = "prob")`              |
|`Weka`       |`RWeka`      |`predict(obj, type = "probability")`       |
|`logitboost` |`LogitBoost` |`predict(obj, type = "raw", nIter)`        |

blatantly stolen from Max Kuhn

---

![:scale 50%](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png)

---

The goals of `parsnip` is...  

- Decouple the *model classification* from the *computational engine*
- Separate the definition of a model from its evaluation
- Harmonize argument names
- Make consistent predictions (always tibbles with na.omit=FALSE)

---


```r
model_glm &lt;- glm(am ~ disp + drat + qsec, data = mtcars, 
                 family = "binomial")
```

---


```r
library(parsnip)
model_glm &lt;- logistic_reg(mode = "classification") %&gt;%
  set_engine("glm")

model_glm
```

```
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm
```

--


```r
fit_glm &lt;- model_glm %&gt;%
  fit(factor(am) ~ disp + drat + qsec, data = mtcars)
```

???
decision_tree(), linear_reg(), logistic_reg(), rand_forest(), svm_poly()

---


```r
library(parsnip)
model_glmnet &lt;- logistic_reg(mode = "classification") %&gt;%
  set_engine("glmnet")
model_glmnet
```

```
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glmnet
```


```r
fit_glmnet &lt;- model_glmnet %&gt;%
  fit(factor(am) ~ disp + drat + qsec, data = mtcars)
```

---

### Using both formula and x/y

#### Formula

```r
fit_glm &lt;- model_glm %&gt;%
  fit(factor(am) ~ ., data = mtcars)
```

#### x/y

```r
fit_glm &lt;- model_glm %&gt;%
  fit_xy(x = as.matrix(mtcars[, c("disp", "drat", "qsec")]),
         y = factor(mtcars[, "am"]), 
         data = mtcars)
```

---

# Tidy prediction


```r
predict(fit_glm, mtcars)
```

```
## # A tibble: 32 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 1          
##  2 1          
##  3 1          
##  4 0          
##  5 0          
##  6 0          
##  7 0          
##  8 0          
##  9 0          
## 10 0          
## # … with 22 more rows
```

---

Consider now that we wanted to model a more advanded relation ship between variables


```r
fit_glm &lt;- model_glm %&gt;%
  fit(factor(am) ~ poly(mpg, 3) + pca(disp:wt)[1] + pca(disp:wt)[2] + pca(disp:wt)[3], 
      data = mtcars)
```

--

- Not all inline functions can be used with formulas
- Having to run some calculations many many times
- Connected to the model, calculations are not saved between models

Post by Max Kuhn about the bad sides of formula
https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/

---

![:scale 45%](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png)

???

Preprocessing - statistics  
Feature engineering - Computer Science

---

### Preprocessing steps

Some of things you may need to deal with before you can start modeling

- Same unit (center and scale)
- Remove correlation (filter and PCA extraction)
- Missing data (imputation)
- Dummy varibles
- Zero Variance

---

### Same units


```r
library(recipes)
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors())
```

### PCA


```r
library(recipes)
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_pca(all_predictors(), threshold = 0.8)
```

### Any combination of steps


```r
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_knnimpute(drat, wt, neighbors = 5) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_pca(all_predictors(), threshold = 0.8)
```

---


```r
library(recipes)
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors())
```

--


```r
car_rec
```

```
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         10
## 
## Operations:
## 
## Centering for all_predictors()
## Scaling for all_predictors()
```

---


```r
library(recipes)
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors())

car_preped &lt;- prep(car_rec, training = mtcars)
```

--


```r
bake(car_preped, new_data = mtcars)
```

--


```
## # A tibble: 32 x 11
##      mpg    cyl    disp     hp   drat       wt   qsec     vs     am   gear
##    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1  21   -0.105 -0.571  -0.535  0.568 -0.610   -0.777 -0.868  1.19   0.424
##  2  21   -0.105 -0.571  -0.535  0.568 -0.350   -0.464 -0.868  1.19   0.424
##  3  22.8 -1.22  -0.990  -0.783  0.474 -0.917    0.426  1.12   1.19   0.424
##  4  21.4 -0.105  0.220  -0.535 -0.966 -0.00230  0.890  1.12  -0.814 -0.932
##  5  18.7  1.01   1.04    0.413 -0.835  0.228   -0.464 -0.868 -0.814 -0.932
##  6  18.1 -0.105 -0.0462 -0.608 -1.56   0.248    1.33   1.12  -0.814 -0.932
##  7  14.3  1.01   1.04    1.43  -0.723  0.361   -1.12  -0.868 -0.814 -0.932
##  8  24.4 -1.22  -0.678  -1.24   0.175 -0.0278   1.20   1.12  -0.814  0.424
##  9  22.8 -1.22  -0.726  -0.754  0.605 -0.0687   2.83   1.12  -0.814  0.424
## 10  19.2 -0.105 -0.509  -0.345  0.605  0.228    0.253  1.12  -0.814  0.424
## # … with 22 more rows, and 1 more variable: carb &lt;dbl&gt;
```

---


```r
library(recipes)
car_rec &lt;- recipe(mpg ~ ., mtcars) %&gt;%
  step_center(all_predictors()) %&gt;%
  step_scale(all_predictors())

car_preped &lt;- prep(car_rec, training = mtcars)
```


```r
juice(car_preped)
```

```
## # A tibble: 32 x 11
##       cyl    disp     hp   drat       wt   qsec     vs     am   gear   carb
##     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 -0.105 -0.571  -0.535  0.568 -0.610   -0.777 -0.868  1.19   0.424  0.735
##  2 -0.105 -0.571  -0.535  0.568 -0.350   -0.464 -0.868  1.19   0.424  0.735
##  3 -1.22  -0.990  -0.783  0.474 -0.917    0.426  1.12   1.19   0.424 -1.12 
##  4 -0.105  0.220  -0.535 -0.966 -0.00230  0.890  1.12  -0.814 -0.932 -1.12 
##  5  1.01   1.04    0.413 -0.835  0.228   -0.464 -0.868 -0.814 -0.932 -0.503
##  6 -0.105 -0.0462 -0.608 -1.56   0.248    1.33   1.12  -0.814 -0.932 -1.12 
##  7  1.01   1.04    1.43  -0.723  0.361   -1.12  -0.868 -0.814 -0.932  0.735
##  8 -1.22  -0.678  -1.24   0.175 -0.0278   1.20   1.12  -0.814  0.424 -0.503
##  9 -1.22  -0.726  -0.754  0.605 -0.0687   2.83   1.12  -0.814  0.424 -0.503
## 10 -0.105 -0.509  -0.345  0.605  0.228    0.253  1.12  -0.814  0.424  0.735
## # … with 22 more rows, and 1 more variable: mpg &lt;dbl&gt;
```

---

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.huge[
.center[

```r
recipe -&gt; prepare -&gt; bake/juice

(define) -&gt; (estimate) -&gt; (apply)
```
]
]

---

## Types of data splitting

- Random
- By date
- By outcome
    - Classification: within class
    - regression: within quantile
    
---

## Training and Testing sets


```r
library(rsample)

car_preped &lt;- prep(car_rec, training = mtcars)
```

---

![:scale 45%](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png)

---

## Training and Testing sets


```r
library(rsample)

set.seed(4595)

# These slides were almost finished and I didn't want to change the data in all the other slides
big_mtcars &lt;- rerun(10, mtcars) %&gt;%
  bind_rows()

data_split &lt;- initial_split(big_mtcars, strata = "mpg", p = 0.80)

# Training and test data
cars_train &lt;- training(data_split)
cars_test  &lt;- testing(data_split)

car_prep &lt;- prep(car_rec, training = cars_train)

# Preprocessed data
cars_train_p &lt;- juice(car_prep)
cars_test_p &lt;-bake(car_prep, new_data = cars_test)
```

---

## Cross-Validating (sneak peak)


```r
set.seed(1234)
cv_splits &lt;- vfold_cv(
  data = big_mtcars, 
  v = 10, 
  strata = "mpg"
)

cv_splits
```

```
## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits           id    
##    &lt;list&gt;           &lt;chr&gt; 
##  1 &lt;split [288/32]&gt; Fold01
##  2 &lt;split [288/32]&gt; Fold02
##  3 &lt;split [288/32]&gt; Fold03
##  4 &lt;split [288/32]&gt; Fold04
##  5 &lt;split [288/32]&gt; Fold05
##  6 &lt;split [288/32]&gt; Fold06
##  7 &lt;split [288/32]&gt; Fold07
##  8 &lt;split [288/32]&gt; Fold08
##  9 &lt;split [288/32]&gt; Fold09
## 10 &lt;split [288/32]&gt; Fold10
```

---


```r
car_form &lt;- mpg ~ disp + qsec + cyl
# Fit on a single analysis resample
fit_model &lt;- function(split, spec) {
  fit(
    object = nearest_neighbor() %&gt;% set_engine("kknn"), 
    formula = car_form,
    data = analysis(split) # &lt;- pull out training set
  )
}
# For each resample, call fit_model()
cv_splits &lt;- cv_splits %&gt;% 
  mutate(models_knn = map(splits, fit_model, spec_lm),
         )
cv_splits
```

```
## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 3
##    splits           id     models_knn
##  * &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;    
##  1 &lt;split [288/32]&gt; Fold01 &lt;fit[+]&gt;  
##  2 &lt;split [288/32]&gt; Fold02 &lt;fit[+]&gt;  
##  3 &lt;split [288/32]&gt; Fold03 &lt;fit[+]&gt;  
##  4 &lt;split [288/32]&gt; Fold04 &lt;fit[+]&gt;  
##  5 &lt;split [288/32]&gt; Fold05 &lt;fit[+]&gt;  
##  6 &lt;split [288/32]&gt; Fold06 &lt;fit[+]&gt;  
##  7 &lt;split [288/32]&gt; Fold07 &lt;fit[+]&gt;  
##  8 &lt;split [288/32]&gt; Fold08 &lt;fit[+]&gt;  
##  9 &lt;split [288/32]&gt; Fold09 &lt;fit[+]&gt;  
## 10 &lt;split [288/32]&gt; Fold10 &lt;fit[+]&gt;
```

---

![:scale 45%](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/yardstick.png)

---


```r
library(yardstick)
head(two_class_example)
```

```
##    truth      Class1       Class2 predicted
## 1 Class2 0.003589243 0.9964107574    Class2
## 2 Class1 0.678621054 0.3213789460    Class1
## 3 Class2 0.110893522 0.8891064779    Class2
## 4 Class1 0.735161703 0.2648382969    Class1
## 5 Class2 0.016239960 0.9837600397    Class2
## 6 Class1 0.999275071 0.0007249286    Class1
```

--


```r
metrics(two_class_example, truth = truth, estimate = predicted)
```

```
## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.838
## 2 kap      binary         0.675
```

---


```r
library(yardstick)
head(two_class_example)
```

```
##    truth      Class1       Class2 predicted
## 1 Class2 0.003589243 0.9964107574    Class2
## 2 Class1 0.678621054 0.3213789460    Class1
## 3 Class2 0.110893522 0.8891064779    Class2
## 4 Class1 0.735161703 0.2648382969    Class1
## 5 Class2 0.016239960 0.9837600397    Class2
## 6 Class1 0.999275071 0.0007249286    Class1
```


```r
accuracy(two_class_example, truth = truth, estimate = predicted)
```

```
## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.838
```

---


```r
library(yardstick)
head(two_class_example)
```

```
##    truth      Class1       Class2 predicted
## 1 Class2 0.003589243 0.9964107574    Class2
## 2 Class1 0.678621054 0.3213789460    Class1
## 3 Class2 0.110893522 0.8891064779    Class2
## 4 Class1 0.735161703 0.2648382969    Class1
## 5 Class2 0.016239960 0.9837600397    Class2
## 6 Class1 0.999275071 0.0007249286    Class1
```


```r
j_index(two_class_example, truth = truth, estimate = predicted)
```

```
## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 j_index binary         0.673
```

And many more!!

---


```r
library(yardstick)
head(two_class_example)
```

```
##    truth      Class1       Class2 predicted
## 1 Class2 0.003589243 0.9964107574    Class2
## 2 Class1 0.678621054 0.3213789460    Class1
## 3 Class2 0.110893522 0.8891064779    Class2
## 4 Class1 0.735161703 0.2648382969    Class1
## 5 Class2 0.016239960 0.9837600397    Class2
## 6 Class1 0.999275071 0.0007249286    Class1
```


```r
conf_mat(two_class_example, truth = truth, estimate = predicted)
```

```
##           Truth
## Prediction Class1 Class2
##     Class1    227     50
##     Class2     31    192
```

---


```r
roc_curve(two_class_example, truth = truth, Class1) %&gt;%
  autoplot()
```

&lt;img src="slides_files/figure-html/unnamed-chunk-44-1.png" width="700px" style="display: block; margin: auto;" /&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
